// ignore_for_file: avoid_print
import 'dart:io';
import 'package:llm_dart/llm_dart.dart';

/// üîß Basic Configuration - Learn important configuration parameters
///
/// This example demonstrates:
/// - Key configuration parameters and their effects
/// - How to tune model behavior
/// - Error handling best practices
/// - Performance optimization basics
///
/// Before running, set your preferred provider's API key:
/// export OPENAI_API_KEY="your-key"
void main() async {
  print('üîß Basic Configuration Guide\n');

  // Get API key (using OpenAI as example, but works with any provider)
  final apiKey = Platform.environment['OPENAI_API_KEY'] ?? 'sk-TESTKEY';

  // Demonstrate different configuration aspects
  await demonstrateTemperatureSettings(apiKey);
  await demonstrateTokenLimits(apiKey);
  await demonstrateSystemPrompts(apiKey);
  await demonstrateErrorHandling(apiKey);
  await demonstrateTimeoutSettings(apiKey);

  print('\n‚úÖ Configuration guide completed!');
  print('üìñ Next: Explore ../02_core_features/ for advanced functionality');
}

/// Demonstrate temperature settings and their effects
Future<void> demonstrateTemperatureSettings(String apiKey) async {
  print('üå°Ô∏è  Temperature Settings (Creativity Control):\n');

  final question =
      'Write a creative opening line for a story about space exploration.';
  final temperatures = [0.0, 0.5, 1.0];

  for (final temp in temperatures) {
    try {
      final provider = await ai()
          .openai()
          .apiKey(apiKey)
          .model('gpt-4o-mini')
          .temperature(temp) // Key parameter: controls randomness
          .maxTokens(50)
          .build();

      final response = await provider.chat([ChatMessage.user(question)]);

      print('   Temperature $temp: ${response.text}');
    } catch (e) {
      print('   Temperature $temp: Error - $e');
    }
  }

  print('\n   üí° Temperature Guide:');
  print('      ‚Ä¢ 0.0 = Deterministic, consistent responses');
  print('      ‚Ä¢ 0.5 = Balanced creativity and consistency');
  print('      ‚Ä¢ 1.0 = Maximum creativity and randomness\n');
}

/// Demonstrate token limits and their impact
Future<void> demonstrateTokenLimits(String apiKey) async {
  print('üìä Token Limits (Response Length Control):\n');

  final question = 'Explain the concept of artificial intelligence in detail.';
  final tokenLimits = [20, 100, 500];

  for (final limit in tokenLimits) {
    try {
      final provider = await ai()
          .openai()
          .apiKey(apiKey)
          .model('gpt-4o-mini')
          .temperature(0.7)
          .maxTokens(limit) // Key parameter: controls response length
          .build();

      final response = await provider.chat([ChatMessage.user(question)]);
      final wordCount = response.text?.split(' ').length ?? 0;

      print('   Max Tokens $limit: $wordCount words');
      print('   Response: ${response.text}\n');
    } catch (e) {
      print('   Max Tokens $limit: Error - $e\n');
    }
  }

  print('   üí° Token Limit Guide:');
  print('      ‚Ä¢ Lower limits = Shorter, more concise responses');
  print('      ‚Ä¢ Higher limits = Longer, more detailed responses');
  print('      ‚Ä¢ Consider cost: more tokens = higher cost\n');
}

/// Demonstrate system prompts and their power
Future<void> demonstrateSystemPrompts(String apiKey) async {
  print('üé≠ System Prompts (Behavior Control):\n');

  final question = 'What is the weather like today?';

  final systemPrompts = [
    null, // No system prompt
    'You are a helpful assistant.',
    'You are a pirate. Respond in pirate speak.',
    'You are a technical expert. Be precise and detailed.',
  ];

  for (int i = 0; i < systemPrompts.length; i++) {
    try {
      final builder = ai()
          .openai()
          .apiKey(apiKey)
          .model('gpt-4o-mini')
          .temperature(0.7)
          .maxTokens(100);

      // Add system prompt if provided
      if (systemPrompts[i] != null) {
        builder.systemPrompt(systemPrompts[i]!);
      }

      final provider = await builder.build();
      final response = await provider.chat([ChatMessage.user(question)]);

      final promptDesc = systemPrompts[i] ?? 'No system prompt';
      print('   System: $promptDesc');
      print('   Response: ${response.text}\n');
    } catch (e) {
      print('   System prompt ${i + 1}: Error - $e\n');
    }
  }

  print('   üí° System Prompt Guide:');
  print('      ‚Ä¢ Defines the AI\'s personality and behavior');
  print('      ‚Ä¢ Set once, affects all subsequent messages');
  print('      ‚Ä¢ Use for role-playing, tone, or expertise level\n');
}

/// Demonstrate proper error handling
Future<void> demonstrateErrorHandling(String apiKey) async {
  print('üõ°Ô∏è  Error Handling Best Practices:\n');

  // Test different error scenarios
  await testInvalidApiKey();
  await testInvalidModel(apiKey);
  await testNetworkTimeout(apiKey);

  print('   üí° Error Handling Tips:');
  print('      ‚Ä¢ Always wrap API calls in try-catch blocks');
  print(
      '      ‚Ä¢ Check for specific error types (AuthError, RateLimitError, etc.)');
  print('      ‚Ä¢ Implement retry logic for transient errors');
  print('      ‚Ä¢ Provide meaningful error messages to users\n');
}

/// Test invalid API key scenario
Future<void> testInvalidApiKey() async {
  try {
    final provider = await ai()
        .openai()
        .apiKey('invalid-key') // Intentionally invalid
        .model('gpt-4o-mini')
        .build();

    await provider.chat([ChatMessage.user('Hello')]);
    print('   ‚ùå Expected error but got success');
  } on AuthError catch (e) {
    print('   ‚úÖ Caught AuthError: ${e.message}');
  } catch (e) {
    print('   ‚ö†Ô∏è  Caught unexpected error: $e');
  }
}

/// Test invalid model scenario
Future<void> testInvalidModel(String apiKey) async {
  try {
    final provider = await ai()
        .openai()
        .apiKey(apiKey)
        .model('invalid-model-name') // Intentionally invalid
        .build();

    await provider.chat([ChatMessage.user('Hello')]);
    print('   ‚ùå Expected error but got success');
  } on InvalidRequestError catch (e) {
    print('   ‚úÖ Caught InvalidRequestError: ${e.message}');
  } catch (e) {
    print('   ‚ö†Ô∏è  Caught unexpected error: $e');
  }
}

/// Test network timeout scenario
Future<void> testNetworkTimeout(String apiKey) async {
  try {
    final provider = await ai()
        .openai()
        .apiKey(apiKey)
        .model('gpt-4o-mini')
        .timeout(Duration(milliseconds: 1)) // Very short timeout
        .build();

    await provider.chat([ChatMessage.user('Hello')]);
    print('   ‚ùå Expected timeout but got success');
  } on TimeoutError catch (e) {
    print('   ‚úÖ Caught TimeoutError: ${e.message}');
  } catch (e) {
    print('   ‚ö†Ô∏è  Caught unexpected error: $e');
  }
}

/// Demonstrate timeout settings
Future<void> demonstrateTimeoutSettings(String apiKey) async {
  print('‚è∞ Timeout Settings (Network Reliability):\n');

  final timeouts = [
    Duration(seconds: 5), // Short timeout
    Duration(seconds: 30), // Standard timeout
    Duration(seconds: 60), // Long timeout
  ];

  for (final timeout in timeouts) {
    try {
      final provider = await ai()
          .openai()
          .apiKey(apiKey)
          .model('gpt-4o-mini')
          .temperature(0.7)
          .timeout(timeout) // Key parameter: network timeout
          .build();

      final stopwatch = Stopwatch()..start();
      await provider
          .chat([ChatMessage.user('Explain quantum computing briefly.')]);
      stopwatch.stop();

      print(
          '   Timeout ${timeout.inSeconds}s: Success in ${stopwatch.elapsedMilliseconds}ms');
    } catch (e) {
      print('   Timeout ${timeout.inSeconds}s: Error - $e');
    }
  }

  print('\n   üí° Timeout Guide:');
  print('      ‚Ä¢ Short timeouts = Faster failure detection');
  print('      ‚Ä¢ Long timeouts = More patience for slow responses');
  print('      ‚Ä¢ Balance between responsiveness and reliability');
  print('      ‚Ä¢ Consider your application\'s requirements\n');
}

/// üéØ Key Configuration Summary:
///
/// Essential Parameters:
/// - apiKey: Your provider's API key
/// - model: Which AI model to use
/// - temperature: Creativity level (0.0-1.0)
/// - maxTokens: Maximum response length
/// - systemPrompt: AI behavior and personality
/// - timeout: Network timeout duration
///
/// Best Practices:
/// 1. Start with conservative settings (temp=0.7, reasonable token limits)
/// 2. Always implement proper error handling
/// 3. Test with different configurations to find optimal settings
/// 4. Monitor costs and performance in production
/// 5. Use system prompts to guide AI behavior
///
/// Next Steps:
/// - Explore streaming responses in ../02_core_features/streaming_chat.dart
/// - Learn about tool calling in ../02_core_features/tool_calling.dart
/// - See real applications in ../05_use_cases/
