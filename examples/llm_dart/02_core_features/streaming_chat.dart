// ignore_for_file: avoid_print
import 'dart:io';
import 'package:llm_dart/llm_dart.dart';

/// üåä Streaming Chat - Real-time Response Streaming
///
/// This example demonstrates how to handle real-time streaming responses:
/// - Processing stream events as they arrive
/// - Building responsive user interfaces
/// - Handling different event types
/// - Error recovery and stream management
///
/// Before running, set your API key:
/// export OPENAI_API_KEY="your-key"
/// export GROQ_API_KEY="your-key"
void main() async {
  print('üåä Streaming Chat - Real-time Response Streaming\n');

  // Get API key
  final apiKey = Platform.environment['GROQ_API_KEY'] ?? 'gsk-TESTKEY';

  // Create AI model (Groq is great for streaming due to speed)
  final model = await ai()
      .groq()
      .apiKey(apiKey)
      .model('llama-3.1-8b-instant')
      .temperature(0.7)
      .maxTokens(500)
      .buildLanguageModel();

  // Demonstrate different streaming scenarios
  await demonstrateBasicStreaming(model);
  await demonstrateStreamEventTypes(model);
  await demonstrateStreamingWithThinking(model);
  await demonstrateStreamErrorHandling(model);
  await demonstrateStreamPerformance(model);

  print('\n‚úÖ Streaming chat completed!');
}

/// Demonstrate basic streaming functionality
Future<void> demonstrateBasicStreaming(LanguageModel model) async {
  print('‚ö° Basic Streaming:\n');

  try {
    final prompt = ChatPromptBuilder.user()
        .text('Count from 1 to 10 and explain each number briefly.')
        .build();

    print('   User: Count from 1 to 10 and explain each number briefly.');
    print('   AI: ');

    // Stream the response
    await for (final event
        in streamTextWithModel(model, promptMessages: [prompt])) {
      switch (event) {
        case TextDeltaEvent(delta: final delta):
          // Print each text chunk as it arrives
          stdout.write(delta);
          break;
        case CompletionEvent():
          // Stream completed
          print('\n   ‚úÖ Basic streaming successful\n');
          break;
        case ErrorEvent(error: final error):
          print('\n   ‚ùå Stream error: $error\n');
          break;
        case ThinkingDeltaEvent():
        case ToolCallDeltaEvent():
          // Handle other event types
          break;
      }
    }
  } catch (e) {
    print('   ‚ùå Basic streaming failed: $e\n');
  }
}

/// Demonstrate different stream event types
Future<void> demonstrateStreamEventTypes(LanguageModel model) async {
  print('üì° Stream Event Types:\n');

  try {
    final prompt = ChatPromptBuilder.user()
        .text('Write a short poem about programming.')
        .build();

    print('   User: Write a short poem about programming.');
    print('   Processing events:\n');

    var textChunks = 0;
    var totalText = '';

    await for (final event
        in streamTextWithModel(model, promptMessages: [prompt])) {
      switch (event) {
        case TextDeltaEvent(delta: final delta):
          textChunks++;
          totalText += delta;
          print('   üìù Text chunk $textChunks: "$delta"');
          break;

        case ThinkingDeltaEvent(delta: final delta):
          print('   üß† Thinking: $delta');
          break;

        case ToolCallDeltaEvent(toolCall: final toolCall):
          print('   üîß Tool call: ${toolCall.function.name}');
          break;

        case CompletionEvent(response: final response):
          print('\n   üèÅ Completion event received');
          if (response.usage != null) {
            print('   üìä Usage: ${response.usage!.totalTokens} tokens');
          }
          break;

        case ErrorEvent(error: final error):
          print('   ‚ùå Error event: $error');
          break;
      }
    }

    print('\n   üìà Stream Statistics:');
    print('      ‚Ä¢ Total text chunks: $textChunks');
    print('      ‚Ä¢ Final text length: ${totalText.length} characters');
    print('   ‚úÖ Event types demonstration successful\n');
  } catch (e) {
    print('   ‚ùå Event types demonstration failed: $e\n');
  }
}

/// Demonstrate streaming with thinking process (if supported)
Future<void> demonstrateStreamingWithThinking(LanguageModel baseModel) async {
  print('üß† Streaming with Thinking Process:\n');

  try {
    // Try with a provider that supports thinking (switch to Anthropic if available)
    final anthropicKey = Platform.environment['ANTHROPIC_API_KEY'];
    LanguageModel thinkingModel = baseModel;

    if (anthropicKey != null && anthropicKey.isNotEmpty) {
      thinkingModel = await ai()
          .anthropic()
          .apiKey(anthropicKey)
          .model('claude-3-5-haiku-20241022')
          .temperature(0.7)
          .buildLanguageModel();
    }

    final prompt = ChatPromptBuilder.user()
        .text('Solve this step by step: What is 15% of 240?')
        .build();

    print('   User: Solve this step by step: What is 15% of 240?');
    print('   Processing with thinking:\n');

    var hasThinking = false;

    await for (final event in streamTextWithModel(
      thinkingModel,
      promptMessages: [prompt],
    )) {
      switch (event) {
        case ThinkingDeltaEvent(delta: final delta):
          hasThinking = true;
          print('   üß† Thinking: $delta');
          break;

        case TextDeltaEvent(delta: final delta):
          stdout.write(delta);
          break;

        case CompletionEvent():
          print('\n');
          break;

        case ErrorEvent(error: final error):
          print('   ‚ùå Error: $error');
          break;

        case ToolCallDeltaEvent():
          // Handle tool calls if needed
          break;
      }
    }

    if (hasThinking) {
      print('   ‚úÖ Thinking process captured during streaming');
    } else {
      print(
          '   ‚ÑπÔ∏è  No thinking process available (provider may not support it)');
    }

    print('   ‚úÖ Thinking demonstration completed\n');
  } catch (e) {
    print('   ‚ùå Thinking demonstration failed: $e\n');
  }
}

/// Demonstrate stream error handling
Future<void> demonstrateStreamErrorHandling(LanguageModel model) async {
  print('üõ°Ô∏è  Stream Error Handling:\n');

  try {
    // Create a provider with invalid settings to trigger errors
    final invalidModel = await ai()
        .openai()
        .apiKey('invalid-key') // Invalid API key
        .model('gpt-4o-mini')
        .buildLanguageModel();

    final prompt = ChatPromptBuilder.user()
        .text('This should fail due to invalid API key.')
        .build();

    print('   Testing error handling with invalid API key...');

    await for (final event
        in streamTextWithModel(invalidModel, promptMessages: [prompt])) {
      switch (event) {
        case TextDeltaEvent(delta: final delta):
          print('   üìù Unexpected text: $delta');
          break;

        case ErrorEvent(error: final error):
          print('   ‚úÖ Caught error in stream: ${error.runtimeType}');
          print('   üìù Error message: ${error.toString()}');
          break;

        case CompletionEvent():
          print('   ‚ùå Unexpected completion');
          break;

        case ThinkingDeltaEvent():
        case ToolCallDeltaEvent():
          // Handle other event types
          break;
      }
    }
  } catch (e) {
    print('   ‚úÖ Caught exception: ${e.runtimeType}');
    print('   üìù Exception message: $e');
  }

  print('\n   üí° Error Handling Best Practices:');
  print('      ‚Ä¢ Always wrap stream processing in try-catch');
  print('      ‚Ä¢ Handle ErrorEvent within the stream');
  print('      ‚Ä¢ Implement retry logic for transient errors');
  print('      ‚Ä¢ Provide user feedback for stream interruptions');
  print('   ‚úÖ Error handling demonstration completed\n');
}

/// Demonstrate stream performance characteristics
Future<void> demonstrateStreamPerformance(LanguageModel model) async {
  print('üöÄ Stream Performance:\n');

  try {
    final prompt = ChatPromptBuilder.user()
        .text('Write a detailed explanation of machine learning in 200 words.')
        .build();

    print(
        '   User: Write a detailed explanation of machine learning in 200 words.');
    print('   Measuring performance...\n');

    final stopwatch = Stopwatch()..start();
    var firstChunkTime = 0;
    var chunkCount = 0;
    var totalChars = 0;
    final chunkTimes = <int>[];

    await for (final event
        in streamTextWithModel(model, promptMessages: [prompt])) {
      switch (event) {
        case TextDeltaEvent(delta: final delta):
          chunkCount++;
          totalChars += delta.length;

          if (firstChunkTime == 0) {
            firstChunkTime = stopwatch.elapsedMilliseconds;
            print('   ‚ö° First chunk received: ${firstChunkTime}ms');
          }

          chunkTimes.add(stopwatch.elapsedMilliseconds);
          break;

        case CompletionEvent():
          stopwatch.stop();
          break;

        case ErrorEvent(error: final error):
          print('   ‚ùå Performance test error: $error');
          return;

        case ThinkingDeltaEvent():
        case ToolCallDeltaEvent():
          // Handle other event types
          break;
      }
    }

    final totalTime = stopwatch.elapsedMilliseconds;
    final avgChunkInterval = chunkTimes.length > 1
        ? (totalTime - firstChunkTime) / (chunkTimes.length - 1)
        : 0;

    print('\n   üìä Performance Metrics:');
    print('      ‚Ä¢ Time to first chunk: ${firstChunkTime}ms');
    print('      ‚Ä¢ Total response time: ${totalTime}ms');
    print('      ‚Ä¢ Total chunks: $chunkCount');
    print('      ‚Ä¢ Total characters: $totalChars');
    print(
        '      ‚Ä¢ Average chunk interval: ${avgChunkInterval.toStringAsFixed(1)}ms');
    print(
        '      ‚Ä¢ Characters per second: ${(totalChars * 1000 / totalTime).toStringAsFixed(1)}');

    print('\n   üí° Performance Benefits:');
    print('      ‚Ä¢ Reduced perceived latency (first chunk arrives quickly)');
    print('      ‚Ä¢ Better user experience (progressive content display)');
    print('      ‚Ä¢ Ability to process content as it arrives');
    print('      ‚Ä¢ Early error detection and handling');

    print('   ‚úÖ Performance demonstration completed\n');
  } catch (e) {
    print('   ‚ùå Performance demonstration failed: $e\n');
  }
}

/// üéØ Key Streaming Concepts Summary:
///
/// Stream Events:
/// - TextDeltaEvent: Incremental text content
/// - ThinkingDeltaEvent: AI reasoning process (some models)
/// - ToolCallDeltaEvent: Function calls (when using tools)
/// - CompletionEvent: Stream completion with metadata
/// - ErrorEvent: Error handling within stream
///
/// Benefits:
/// - Reduced perceived latency
/// - Real-time user feedback
/// - Progressive content display
/// - Better error handling
///
/// Best Practices:
/// 1. Handle all event types appropriately
/// 2. Implement proper error handling
/// 3. Measure and optimize performance
/// 4. Provide user feedback during streaming
/// 5. Consider buffering for UI updates
///
/// Next Steps:
/// - tool_calling.dart: Function calling and execution
/// - structured_output.dart: JSON schema responses
/// - error_handling.dart: Production-ready error management
