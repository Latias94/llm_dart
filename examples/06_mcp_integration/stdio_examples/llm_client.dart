// ignore_for_file: avoid_print
import 'dart:io';
import 'dart:convert';
import 'package:llm_dart/llm_dart.dart';
import 'package:llm_dart_core/llm_dart_core.dart'
    show
        ChatContentPart,
        TextContentPart,
        ToolCallContentPart,
        ToolResultContentPart,
        ToolResultTextPayload;
import 'package:mcp_dart/mcp_dart.dart' hide Tool;

/// stdio LLM Integration - AI Agents with stdio MCP Tools
///
/// This example demonstrates how to integrate LLMs with MCP servers
/// using stdio transport. The LLM can discover and use tools from
/// the stdio MCP server through real MCP client-server communication.
///
/// Architecture:
/// LLM (OpenAI/etc) ‚Üî llm_dart ‚Üî Real MCP Client ‚Üî stdio MCP Server ‚Üî Tools
///
/// Before running:
/// 1. Start the MCP server: dart run 06_mcp_integration/stdio_examples/server.dart
/// 2. Set API key: export OPENAI_API_KEY="your-key-here"
/// 3. Run this client: dart run 06_mcp_integration/stdio_examples/llm_client.dart
void main() async {
  print('ü§ñ stdio LLM Integration - AI Agents with Real stdio MCP Tools\n');

  // Get API key
  final apiKey = Platform.environment['OPENAI_API_KEY'] ?? 'sk-TESTKEY';
  if (apiKey == 'sk-TESTKEY') {
    print(
        '‚ö†Ô∏è  Warning: Using test API key. Set OPENAI_API_KEY for real usage.\n');
  }

  await demonstrateBasicStdioIntegration(apiKey);
  await demonstrateStdioCalculationWorkflow(apiKey);
  await demonstrateStdioMultiToolWorkflow(apiKey);

  print('\n‚úÖ stdio LLM integration examples completed!');
  print('üöÄ You can now build AI agents that use real stdio MCP tools!');
}

/// Demonstrate basic stdio MCP + LLM integration
Future<void> demonstrateBasicStdioIntegration(String apiKey) async {
  print('üîó Basic stdio MCP + LLM Integration:\n');

  Client? mcpClient;
  try {
    // Create high-level language model
    final model = await ai()
        .openai()
        .apiKey(apiKey)
        .model('gpt-4o-mini')
        .temperature(0.7)
        .buildLanguageModel();

    // Create real MCP client connected to stdio server
    mcpClient = await _createRealStdioMcpClient();

    // Get MCP tools from real server
    final mcpTools = await _getMcpToolsAsLlmDartTools(mcpClient);

    print('   üîß Available stdio MCP Tools:');
    for (final tool in mcpTools) {
      print('      ‚Ä¢ ${tool.function.name}: ${tool.function.description}');
    }

    // Test with a simple calculation request
    const userMessage = 'Calculate 25 * 8 + 12 using the available tools.';
    final prompt = ChatPromptBuilder.user().text(userMessage).build();

    // Print actual user message
    print('   üí¨ User Message:');
    print('      "$userMessage"');
    print('   ü§ñ LLM: Analyzing request and selecting appropriate tools...');

    final response = await generateTextWithModel(
      model,
      promptMessages: [prompt],
      options: LanguageModelCallOptions(tools: mcpTools),
    );

    if (response.toolCalls != null && response.toolCalls!.isNotEmpty) {
      print('   ü§ñ LLM: Requested ${response.toolCalls!.length} tool call(s):');

      // Execute MCP tools and collect results
      final toolResultCalls = <ToolCall>[];
      for (int i = 0; i < response.toolCalls!.length; i++) {
        final toolCall = response.toolCalls![i];
        print('      ${i + 1}. üõ†Ô∏è  Tool Call:');
        print('         üìû Function: ${toolCall.function.name}');
        print('         üìã Arguments: ${toolCall.function.arguments}');
        print('         üÜî Call ID: ${toolCall.id}');

        // Execute the MCP tool via real stdio client
        final mcpResult = await _executeRealMcpTool(
          mcpClient,
          toolCall.function.name,
          toolCall.function.arguments,
        );

        // Create tool result call
        toolResultCalls.add(ToolCall(
          id: toolCall.id,
          callType: 'function',
          function: FunctionCall(
            name: toolCall.function.name,
            arguments: mcpResult,
          ),
        ));
      }

      // Build prompt-first conversation with tool calls and results.
      final conversation = <ModelMessage>[
        prompt,
        _assistantWithToolCalls(response.text, response.toolCalls!),
      ];

      for (final toolCall in toolResultCalls) {
        conversation
            .add(_toolResultMessage(toolCall, toolCall.function.arguments));
      }

      // Send tool results back to LLM for final response
      print('   üîÑ Sending MCP results back to LLM for final response...');
      final finalResult = await generateTextWithModel(
        model,
        promptMessages: conversation,
      );
      print('   üìù LLM Final Response: ${finalResult.text}');
    } else {
      print('   üìù LLM Response: ${response.text}');
    }
    print('   ‚úÖ Basic stdio integration successful\n');
  } catch (e) {
    print('   ‚ùå Basic stdio integration failed: $e\n');
  } finally {
    await mcpClient?.close();
  }
}

/// Demonstrate stdio calculation workflow
Future<void> demonstrateStdioCalculationWorkflow(String apiKey) async {
  print('üßÆ stdio Calculation Workflow:\n');

  Client? mcpClient;
  try {
    final model = await ai()
        .openai()
        .apiKey(apiKey)
        .model('gpt-4o-mini')
        .temperature(0.3)
        .buildLanguageModel();

    // Create real MCP client connected to stdio server
    mcpClient = await _createRealStdioMcpClient();
    final mcpTools = await _getMcpToolsAsLlmDartTools(mcpClient);

    // Mathematical workflow request (prompt-first)
    const systemPrompt =
        'You are a math assistant that can use calculation tools. '
        'The calculation tool supports basic operations: +, -, *, /. '
        'It does NOT support ^ (power), pi, sqrt, or other advanced functions. '
        'Use only basic arithmetic operations and break down complex calculations into simple steps.';
    const userPrompt =
        'I need to calculate the area of a circle with radius 7 (use 3.14159 for pi), '
        'then find what percentage that area is of a square with side length 20. '
        'Please use only basic arithmetic operations (+, -, *, /) in your calculations.';

    final prompts = <ModelMessage>[
      ChatPromptBuilder.system().text(systemPrompt).build(),
      ChatPromptBuilder.user().text(userPrompt).build(),
    ];

    // Print actual user message
    print('   üí¨ User Message:');
    print('      "$userPrompt"');
    print('   ü§ñ LLM: Breaking down the mathematical workflow...');

    final response = await generateTextWithModel(
      model,
      promptMessages: prompts,
      options: LanguageModelCallOptions(tools: mcpTools),
    );

    print('   üìã stdio calculation workflow execution:');
    if (response.toolCalls != null && response.toolCalls!.isNotEmpty) {
      // Execute MCP tools and collect results
      final toolResultCalls = <ToolCall>[];
      for (int i = 0; i < response.toolCalls!.length; i++) {
        final toolCall = response.toolCalls![i];
        print('      Step ${i + 1}: üõ†Ô∏è  Tool Call:');
        print('         üìû Function: ${toolCall.function.name}');
        print('         üìã Arguments: ${toolCall.function.arguments}');
        print('         üÜî Call ID: ${toolCall.id}');

        final mcpResult = await _executeRealMcpTool(
          mcpClient,
          toolCall.function.name,
          toolCall.function.arguments,
        );

        // Create tool result call
        toolResultCalls.add(ToolCall(
          id: toolCall.id,
          callType: 'function',
          function: FunctionCall(
            name: toolCall.function.name,
            arguments: mcpResult,
          ),
        ));
      }

      // Send tool results back to LLM for final response (prompt-first)
      print(
          '   üîÑ Sending MCP results back to LLM for calculation final response...');
      final conversation = <ModelMessage>[
        ...prompts,
        _assistantWithToolCalls(response.text, response.toolCalls!),
      ];

      for (final toolCall in toolResultCalls) {
        conversation
            .add(_toolResultMessage(toolCall, toolCall.function.arguments));
      }

      final finalResult = await generateTextWithModel(
        model,
        promptMessages: conversation,
      );
      print('   üìù Final Response: ${finalResult.text}');
    } else {
      print('   üìù Final Response: ${response.text}');
    }
    print('   ‚úÖ stdio calculation workflow successful\n');
  } catch (e) {
    print('   ‚ùå stdio calculation workflow failed: $e\n');
  } finally {
    await mcpClient?.close();
  }
}

/// Demonstrate multi-tool workflow with stdio
Future<void> demonstrateStdioMultiToolWorkflow(String apiKey) async {
  print('‚ö° stdio Multi-Tool Workflow:\n');

  Client? mcpClient;
  try {
    final model = await ai()
        .openai()
        .apiKey(apiKey)
        .model('gpt-4o-mini')
        .temperature(0.2)
        .buildLanguageModel();

    // Create real MCP client connected to stdio server
    mcpClient = await _createRealStdioMcpClient();
    final mcpTools = await _getMcpToolsAsLlmDartTools(mcpClient);

    // Multi-tool request (prompt-first)
    const systemPrompt =
        'You are a helpful assistant that can use various tools. '
        'The calculation tool supports basic operations: +, -, *, /. '
        'Use multiple tools to gather information and provide comprehensive answers. '
        'Always use the calculation tool for mathematical operations.';
    const userPrompt =
        'Please: 1) Get the current time, 2) Generate a random number between 1-10, '
        '3) Use the calculation tool to calculate that number squared (multiply the number by itself), and 4) Generate a UUID for this session.';

    final prompts = <ModelMessage>[
      ChatPromptBuilder.system().text(systemPrompt).build(),
      ChatPromptBuilder.user().text(userPrompt).build(),
    ];

    // Print actual user message
    print('   üí¨ User Message:');
    print('      "$userPrompt"');
    print('   ü§ñ LLM: Planning multi-tool workflow via stdio...');

    final response = await generateTextWithModel(
      model,
      promptMessages: prompts,
      options: LanguageModelCallOptions(tools: mcpTools),
    );

    print('   üìã Multi-tool workflow execution via stdio:');
    if (response.toolCalls != null && response.toolCalls!.isNotEmpty) {
      // Execute MCP tools and collect results
      final toolResultCalls = <ToolCall>[];
      for (int i = 0; i < response.toolCalls!.length; i++) {
        final toolCall = response.toolCalls![i];
        print('      Step ${i + 1}: üõ†Ô∏è  Tool Call:');
        print('         üìû Function: ${toolCall.function.name}');
        print('         üìã Arguments: ${toolCall.function.arguments}');
        print('         üÜî Call ID: ${toolCall.id}');

        final mcpResult = await _executeRealMcpTool(
          mcpClient,
          toolCall.function.name,
          toolCall.function.arguments,
        );

        // Create tool result call
        toolResultCalls.add(ToolCall(
          id: toolCall.id,
          callType: 'function',
          function: FunctionCall(
            name: toolCall.function.name,
            arguments: mcpResult,
          ),
        ));
      }

      // Send tool results back to LLM for final response (prompt-first)
      print(
          '   üîÑ Sending MCP results back to LLM for multi-tool final response...');
      final conversation = <ModelMessage>[
        ...prompts,
        _assistantWithToolCalls(response.text, response.toolCalls!),
      ];

      for (final toolCall in toolResultCalls) {
        conversation
            .add(_toolResultMessage(toolCall, toolCall.function.arguments));
      }

      final finalResult = await generateTextWithModel(
        model,
        promptMessages: conversation,
      );
      print('   üìù Final Response: ${finalResult.text}');
    } else {
      print('   üìù Final Response: ${response.text}');
    }
    print('   ‚úÖ stdio multi-tool workflow successful\n');
  } catch (e) {
    print('   ‚ùå stdio multi-tool workflow failed: $e\n');
  } finally {
    await mcpClient?.close();
  }
}

/// Helper to build an assistant message that includes tool call parts.
ModelMessage _assistantWithToolCalls(String? text, List<ToolCall> toolCalls) {
  final parts = <ChatContentPart>[];
  if (text != null && text.isNotEmpty) {
    parts.add(TextContentPart(text));
  }
  for (final call in toolCalls) {
    parts.add(
      ToolCallContentPart(
        toolName: call.function.name,
        argumentsJson: call.function.arguments,
        toolCallId: call.id,
      ),
    );
  }
  return ModelMessage(role: ChatRole.assistant, parts: parts);
}

/// Helper to build a tool result message from a tool call and text result.
ModelMessage _toolResultMessage(ToolCall toolCall, String result) {
  return ModelMessage(
    role: ChatRole.assistant,
    parts: [
      ToolResultContentPart(
        toolCallId: toolCall.id,
        toolName: toolCall.function.name,
        payload: ToolResultTextPayload(result),
      ),
    ],
  );
}

/// Create real MCP client connected to stdio server
Future<Client> _createRealStdioMcpClient() async {
  print('   üîå Creating real stdio MCP client...');

  // Define the server executable and arguments
  const serverCommand = 'dart';
  const serverArgs = <String>[
    'run',
    '06_mcp_integration/stdio_examples/server.dart'
  ];

  // Create StdioServerParameters
  final serverParams = StdioServerParameters(
    command: serverCommand,
    args: serverArgs,
    stderrMode: ProcessStartMode.normal,
  );

  // Create the StdioClientTransport
  final transport = StdioClientTransport(serverParams);

  // Define client information
  final clientInfo =
      Implementation(name: 'LlmDartStdioClient', version: '1.0.0');

  // Create the MCP client
  final client = Client(clientInfo);

  // Set up error and close handlers
  transport.onerror = (error) {
    print('   ‚ùå MCP Transport error: $error');
  };

  transport.onclose = () {
    print('   üîå MCP Transport closed');
  };

  // Connect to the server
  print('   üîó Connecting to stdio MCP server...');
  await client.connect(transport);
  print('   ‚úÖ Connected to stdio MCP server');

  return client;
}

/// Get MCP tools from real server and convert to llm_dart tools
Future<List<Tool>> _getMcpToolsAsLlmDartTools(Client mcpClient) async {
  print('   üîç Discovering MCP tools from real server...');

  try {
    // Get tools from real MCP server
    final toolsResponse = await mcpClient.listTools();
    final mcpTools = toolsResponse.tools;

    print('   ‚úÖ Discovered ${mcpTools.length} MCP tools');

    // Convert MCP tools to llm_dart tools
    final llmDartTools = <Tool>[];

    for (final mcpTool in mcpTools) {
      try {
        // Convert MCP tool schema to ParametersSchema
        final parametersSchema =
            _convertMcpSchemaToParametersSchema(mcpTool.inputSchema.toJson());

        // Convert MCP tool to llm_dart tool
        final llmDartTool = Tool.function(
          name: mcpTool.name,
          description: mcpTool.description ?? 'MCP tool: ${mcpTool.name}',
          parameters: parametersSchema,
        );
        llmDartTools.add(llmDartTool);
      } catch (e) {
        print('   ‚ö†Ô∏è Failed to convert tool ${mcpTool.name}: $e');
      }
    }

    return llmDartTools;
  } catch (error) {
    print('   ‚ùå Error getting MCP tools: $error');
    return [];
  }
}

/// Convert MCP input schema to llm_dart ParametersSchema
ParametersSchema _convertMcpSchemaToParametersSchema(
    Map<String, dynamic>? mcpSchema) {
  if (mcpSchema == null || mcpSchema.isEmpty) {
    return ParametersSchema(
      schemaType: 'object',
      properties: {},
      required: [],
    );
  }

  final properties = <String, ParameterProperty>{};
  final mcpProperties = mcpSchema['properties'] as Map<String, dynamic>? ?? {};

  for (final entry in mcpProperties.entries) {
    final propName = entry.key;
    final propDef = entry.value as Map<String, dynamic>;

    properties[propName] = ParameterProperty(
      propertyType: propDef['type'] as String? ?? 'string',
      description: propDef['description'] as String? ?? '',
    );
  }

  return ParametersSchema(
    schemaType: mcpSchema['type'] as String? ?? 'object',
    properties: properties,
    required: (mcpSchema['required'] as List<dynamic>?)?.cast<String>() ?? [],
  );
}

/// Execute MCP tool via real client and return result
Future<String> _executeRealMcpTool(
    Client mcpClient, String toolName, dynamic arguments) async {
  try {
    print('      üîß MCP: Executing real tool "$toolName"');
    print('         üì• MCP Args: ${jsonEncode(arguments)}');

    // Parse arguments from JSON string
    Map<String, dynamic> parsedArguments = {};
    if (arguments is String && arguments.isNotEmpty && arguments != '{}') {
      try {
        parsedArguments = jsonDecode(arguments) as Map<String, dynamic>;
      } catch (e) {
        print('         ‚ö†Ô∏è Error parsing JSON arguments: $e, using empty args');
        print('         üìã Raw arguments: $arguments');
      }
    } else if (arguments is Map<String, dynamic>) {
      parsedArguments = arguments;
    }

    print(
        '         üì° Executing MCP tool: $toolName with args: $parsedArguments');

    // Execute real MCP tool
    final result = await mcpClient.callTool(
      CallToolRequestParams(
        name: toolName,
        arguments: parsedArguments,
      ),
    );

    // Convert result to string
    final resultText = result.content
        .whereType<TextContent>()
        .map((item) => item.text)
        .join('\n');

    print('         ‚úÖ MCP tool result: $resultText');
    return resultText;
  } catch (error) {
    print('         ‚ùå Error executing MCP tool $toolName: $error');
    return 'Error: $error';
  }
}

/// üéØ Key stdio Integration Concepts:
///
/// stdio MCP Benefits:
/// - Simple process-based architecture
/// - Easy to debug and monitor
/// - Natural fit for local AI tools
/// - Minimal network overhead
///
/// stdio Integration Pattern:
/// 1. Spawn MCP server process
/// 2. Create stdio transport connection
/// 3. Initialize MCP client with transport
/// 4. Discover and convert tools
/// 5. Integrate with LLM tool calling
///
/// Best Practices:
/// 1. Handle server process lifecycle properly
/// 2. Implement proper error handling and recovery
/// 3. Monitor server stderr for debugging
/// 4. Use timeouts for tool execution
/// 5. Clean up processes on exit
///
/// Use Cases:
/// - Local AI assistants
/// - Development and testing tools
/// - Command-line AI applications
/// - Educational examples
/// - Simple automation scripts
///
/// Next Steps:
/// - Implement real stdio transport connection
/// - Add process management utilities
/// - Try HTTP examples for web scenarios
/// - Build production CLI tools
