import 'dart:io';

import 'package:llm_dart_builder/llm_dart_builder.dart';
import 'package:llm_dart_core/llm_dart_core.dart';
import 'package:llm_dart_elevenlabs/llm_dart_elevenlabs.dart';

/// ElevenLabs Audio Tasks Example
///
/// This example demonstrates task-specific audio capabilities (Vercel-aligned):
/// - Text-to-Speech (TTS)
/// - Speech-to-Text (STT) (if available)
/// - Streaming TTS (if available)
/// - Realtime audio sessions (if available)
///
/// Note: Prefer task-level capabilities (TTS/STT/streaming) for composability.
Future<void> main() async {
  final apiKey = Platform.environment['ELEVENLABS_API_KEY'];
  if (apiKey == null || apiKey.isEmpty) {
    print('‚ùå Please set ELEVENLABS_API_KEY environment variable');
    return;
  }

  print('üéôÔ∏è ElevenLabs Audio Tasks Demo\n');

  registerElevenLabs();

  final builder = LLMBuilder()
      .provider(elevenLabsProviderId)
      .apiKey(apiKey)
      .providerOptions(elevenLabsProviderId, const {
    'voiceId': 'JBFqnCBsd6RMkjVDRZzb',
    'stability': 0.5,
    'similarityBoost': 0.75,
    'style': 0.2,
  });

  final ttsProvider = await builder.buildSpeech();

  StreamingTextToSpeechCapability? streamingTtsProvider;
  try {
    streamingTtsProvider = await builder.buildStreamingSpeech();
  } catch (_) {
    streamingTtsProvider = null;
  }

  SpeechToTextCapability? sttProvider;
  try {
    sttProvider = await builder.buildTranscription();
  } catch (_) {
    sttProvider = null;
  }

  RealtimeAudioCapability? realtimeProvider;
  try {
    realtimeProvider = await builder.buildRealtimeAudio();
  } catch (_) {
    realtimeProvider = null;
  }

  await displayCapabilities(
    ttsProvider,
    streamingTtsProvider: streamingTtsProvider,
    sttProvider: sttProvider,
    realtimeProvider: realtimeProvider,
  );

  await testTextToSpeech(ttsProvider);

  if (streamingTtsProvider != null) {
    await testStreamingTextToSpeech(streamingTtsProvider);
  }

  if (sttProvider != null) {
    await testSpeechToText(sttProvider);
  }

  if (realtimeProvider != null) {
    await testRealtimeAudio(realtimeProvider);
  }

  print('‚úÖ ElevenLabs audio tasks demo completed!');
}

Future<void> displayCapabilities(
  TextToSpeechCapability ttsProvider, {
  StreamingTextToSpeechCapability? streamingTtsProvider,
  SpeechToTextCapability? sttProvider,
  RealtimeAudioCapability? realtimeProvider,
}) async {
  print('üîç Available Capabilities:');
  print('   ‚úÖ Text-to-Speech');
  print('   ${streamingTtsProvider == null ? "‚è≠Ô∏è" : "‚úÖ"} Streaming TTS');
  print('   ${sttProvider == null ? "‚è≠Ô∏è" : "‚úÖ"} Speech-to-Text');
  print('   ${realtimeProvider == null ? "‚è≠Ô∏è" : "‚úÖ"} Realtime audio');

  final VoiceListingCapability? voiceListing =
      ttsProvider is VoiceListingCapability
          ? (ttsProvider as VoiceListingCapability)
          : null;
  if (voiceListing != null) {
    try {
      final voices = await voiceListing.getVoices();
      print('   ‚úÖ Voice listing (${voices.length} voices)');
    } catch (_) {
      print('   ‚ö†Ô∏è Voice listing (failed)');
    }
  } else {
    print('   ‚è≠Ô∏è Voice listing (not exposed)');
  }

  print('');
}

Future<void> testTextToSpeech(TextToSpeechCapability provider) async {
  print('üéµ Testing Text-to-Speech');

  try {
    List<VoiceInfo> voices = const [];
    final VoiceListingCapability? voiceListing =
        provider is VoiceListingCapability
            ? (provider as VoiceListingCapability)
            : null;
    if (voiceListing != null) {
      voices = await voiceListing.getVoices();
      print('   üì¢ Available voices: ${voices.length} voices');
      if (voices.isNotEmpty) {
        print(
            '   üé≠ Sample voices: ${voices.take(3).map((v) => v.name).join(', ')}...');
      }
    }

    print('   üîÑ Generating high-quality speech...');
    final highQualityTTS = await provider.textToSpeech(TTSRequest(
      text: 'Welcome to ElevenLabs, the most advanced text-to-speech platform.',
      voice: voices.isNotEmpty ? voices.first.id : 'JBFqnCBsd6RMkjVDRZzb',
      model: 'eleven_multilingual_v2',
      format: 'mp3_44100_128',
      includeTimestamps: true,
      timestampGranularity: TimestampGranularity.character,
      textNormalization: TextNormalization.auto,
      enableLogging: true,
    ));

    await File('elevenlabs_quality.mp3').writeAsBytes(highQualityTTS.audioData);
    print(
        '   ‚úÖ High-quality TTS: ${highQualityTTS.audioData.length} bytes ‚Üí elevenlabs_quality.mp3');

    if (highQualityTTS.alignment != null) {
      final alignment = highQualityTTS.alignment!;
      print(
          '   ‚è±Ô∏è  Character timing: ${alignment.characters.length} characters');
      print('   üìä Sample timing (first 5 chars):');
      for (int i = 0; i < 5 && i < alignment.characters.length; i++) {
        print(
            '      "${alignment.characters[i]}" at ${alignment.characterStartTimes[i]}s');
      }
    }
  } catch (e) {
    print('   ‚ùå TTS failed: $e');
  }

  print('');
}

Future<void> testStreamingTextToSpeech(
  StreamingTextToSpeechCapability provider,
) async {
  print('üì° Testing Streaming Text-to-Speech');

  try {
    final audioChunks = <int>[];
    var chunkCount = 0;

    await for (final event in provider.textToSpeechStream(const TTSRequest(
      text: 'This is a streaming test for ElevenLabs capabilities.',
      processingMode: AudioProcessingMode.streaming,
      optimizeStreamingLatency: 2,
    ))) {
      if (event is AudioDataEvent) {
        audioChunks.addAll(event.data);
        chunkCount++;
        print('   üì¶ Chunk $chunkCount: ${event.data.length} bytes');
        if (event.isFinal) {
          print('   ‚úÖ Streaming complete');
          break;
        }
      } else if (event is AudioTimingEvent) {
        print('   ‚è±Ô∏è  Character "${event.character}" at ${event.startTime}s');
      }
    }

    await File('elevenlabs_streaming.mp3').writeAsBytes(audioChunks);
    print(
        '   ‚úÖ Streaming TTS: $chunkCount chunks, ${audioChunks.length} total bytes ‚Üí elevenlabs_streaming.mp3');
  } catch (e) {
    print('   ‚ùå Streaming TTS failed: $e');
  }

  print('');
}

Future<void> testSpeechToText(SpeechToTextCapability provider) async {
  print('üé§ Testing Speech-to-Text');

  try {
    final TranscriptionLanguageListingCapability? languageListing =
        provider is TranscriptionLanguageListingCapability
            ? (provider as TranscriptionLanguageListingCapability)
            : null;
    if (languageListing != null) {
      final languages = await languageListing.getSupportedLanguages();
      print('   üåç Supported languages: ${languages.length} languages');
      if (languages.isNotEmpty) {
        print(
            '   üó£Ô∏è  Sample languages: ${languages.take(5).map((l) => l.name).join(', ')}...');
      }
    }

    if (await File('elevenlabs_quality.mp3').exists()) {
      print('   üîÑ Transcribing generated audio with advanced features...');

      final advancedSTT = await provider.speechToText(STTRequest.fromFile(
        'elevenlabs_quality.mp3',
        model: 'scribe_v1',
        diarize: true,
        numSpeakers: 1,
        timestampGranularity: TimestampGranularity.word,
        tagAudioEvents: true,
        enableLogging: true,
      ));

      print('   üìù Transcription: "${advancedSTT.text}"');
      print('   üåç Language: ${advancedSTT.language ?? "unknown"}');
      print(
          '   üìä Confidence: ${advancedSTT.languageProbability ?? "unknown"}');

      final words = advancedSTT.words;
      if (words != null && words.isNotEmpty) {
        print('   ‚è±Ô∏è  Word timing (first 3 words):');
        for (final word in words.take(3)) {
          if (word is EnhancedWordTiming) {
            final speaker =
                word.speakerId != null ? ' [${word.speakerId}]' : '';
            print(
                '      "${word.word}"$speaker (${word.start}s - ${word.end}s)');
          } else {
            print('      "${word.word}" (${word.start}s - ${word.end}s)');
          }
        }
      }
    } else {
      print(
          '   ‚ö†Ô∏è  No audio file found for transcription test (elevenlabs_quality.mp3)');
    }
  } catch (e) {
    print('   ‚ùå STT failed: $e');
  }

  print('');
}

Future<void> testRealtimeAudio(RealtimeAudioCapability provider) async {
  print('üéß Testing Realtime Audio Session');

  try {
    final session = await provider.startRealtimeSession(
      const RealtimeAudioConfig(
        enableVAD: true,
        enableEchoCancellation: true,
        enableNoiseSuppression: true,
      ),
    );

    print('   ‚úÖ Real-time session started: ${session.sessionId}');
    session.sendAudio([1, 2, 3, 4, 5]);

    try {
      await session.events.take(1).timeout(const Duration(seconds: 2)).toList();
    } catch (_) {
      // Timeout is expected for this demo.
    }

    await session.close();
    print('   ‚úÖ Real-time session closed');
  } catch (e) {
    print('   ‚ùå Real-time audio failed: $e');
  }

  print('');
}
