// ignore_for_file: avoid_print
import 'dart:io';
import 'package:llm_dart/llm_dart.dart';

/// üü¢ Groq Basic Usage - Ultra-Fast AI Inference
///
/// This example demonstrates the fundamental usage of Groq's lightning-fast models:
/// - Model selection for speed vs quality
/// - Basic chat functionality
/// - Performance benchmarking
/// - Best practices for Groq
///
/// Before running, set your API key:
/// export GROQ_API_KEY="your-groq-api-key"
void main() async {
  print('üü¢ Groq Basic Usage - Ultra-Fast AI Inference\n');

  // Get API key
  final apiKey = Platform.environment['GROQ_API_KEY'] ?? 'gsk-TESTKEY';

  // Demonstrate different Groq usage patterns
  await demonstrateModelSelection(apiKey);
  await demonstrateSpeedBenchmark(apiKey);
  await demonstrateBasicChat(apiKey);
  await demonstrateStreamingPerformance(apiKey);
  await demonstrateBestPractices(apiKey);

  print('\n‚úÖ Groq basic usage completed!');
  print(
      'üìñ Next: Try speed_optimization.dart for advanced performance techniques');
}

/// Demonstrate different Groq models
Future<void> demonstrateModelSelection(String apiKey) async {
  print('üéØ Model Selection:\n');

  final models = [
    {
      'name': 'llama-3.1-8b-instant',
      'description': 'Fastest model, good quality'
    },
    {
      'name': 'llama-3.1-70b-versatile',
      'description': 'Higher quality, still fast'
    },
    {
      'name': 'mixtral-8x7b-32768',
      'description': 'Large context, multilingual'
    },
    {'name': 'gemma-7b-it', 'description': 'Instruction following'},
  ];

  final question = 'Explain artificial intelligence in 2 sentences.';

  for (final model in models) {
    try {
      print('   Testing ${model['name']}: ${model['description']}');

      final provider = await ai()
          .groq()
          .apiKey(apiKey)
          .model(model['name']!)
          .temperature(0.7)
          .maxTokens(100)
          .build();

      final stopwatch = Stopwatch()..start();
      final response = await provider.chat([ChatMessage.user(question)]);
      stopwatch.stop();

      print('      Response: ${response.text}');
      print('      Time: ${stopwatch.elapsedMilliseconds}ms');

      if (response.usage != null) {
        print('      Tokens: ${response.usage!.totalTokens}');
      }

      print('');
    } catch (e) {
      print('      ‚ùå Error with ${model['name']}: $e\n');
    }
  }

  print('   üí° Model Selection Tips:');
  print('      ‚Ä¢ llama-3.1-8b-instant: Best for speed-critical applications');
  print('      ‚Ä¢ llama-3.1-70b-versatile: When you need higher quality');
  print('      ‚Ä¢ mixtral-8x7b-32768: For multilingual or large context needs');
  print('      ‚Ä¢ gemma-7b-it: Good for instruction-following tasks');
  print('   ‚úÖ Model selection demonstration completed\n');
}

/// Demonstrate speed benchmarking
Future<void> demonstrateSpeedBenchmark(String apiKey) async {
  print('‚ö° Speed Benchmark:\n');

  try {
    // Create fastest provider
    final provider = await ai()
        .groq()
        .apiKey(apiKey)
        .model('llama-3.1-8b-instant')
        .temperature(0.7)
        .maxTokens(200)
        .build();

    final testQuestions = [
      'What is machine learning?',
      'Explain quantum computing.',
      'What is blockchain?',
      'Define artificial intelligence.',
      'What is cloud computing?',
    ];

    print(
        '   Running speed benchmark with ${testQuestions.length} questions...');

    final times = <int>[];

    for (int i = 0; i < testQuestions.length; i++) {
      final stopwatch = Stopwatch()..start();

      final response =
          await provider.chat([ChatMessage.user(testQuestions[i])]);

      stopwatch.stop();
      times.add(stopwatch.elapsedMilliseconds);

      print('   ${i + 1}. ${testQuestions[i]}');
      print('      Time: ${stopwatch.elapsedMilliseconds}ms');
      print(
          '      Response: ${response.text?.substring(0, response.text!.length > 80 ? 80 : response.text!.length)}...\n');
    }

    // Calculate statistics
    final avgTime = times.reduce((a, b) => a + b) / times.length;
    final minTime = times.reduce((a, b) => a < b ? a : b);
    final maxTime = times.reduce((a, b) => a > b ? a : b);

    print('   üìä Speed Statistics:');
    print('      ‚Ä¢ Average response time: ${avgTime.toStringAsFixed(1)}ms');
    print('      ‚Ä¢ Fastest response: ${minTime}ms');
    print('      ‚Ä¢ Slowest response: ${maxTime}ms');
    print(
        '      ‚Ä¢ Consistency: ${((maxTime - minTime) / avgTime * 100).toStringAsFixed(1)}% variation');

    print('\n   üöÄ Groq Speed Advantages:');
    print('      ‚Ä¢ Sub-second responses for most queries');
    print('      ‚Ä¢ Consistent low latency');
    print('      ‚Ä¢ Excellent for real-time applications');
    print('      ‚Ä¢ High throughput capabilities');
    print('   ‚úÖ Speed benchmark completed\n');
  } catch (e) {
    print('   ‚ùå Speed benchmark failed: $e\n');
  }
}

/// Demonstrate basic chat functionality
Future<void> demonstrateBasicChat(String apiKey) async {
  print('üí¨ Basic Chat Functionality:\n');

  try {
    // Create Groq provider
    final provider = await ai()
        .groq()
        .apiKey(apiKey)
        .model('llama-3.1-8b-instant')
        .temperature(0.7)
        .maxTokens(300)
        .build();

    // Single message
    print('   Single Message:');
    var response =
        await provider.chat([ChatMessage.user('Write a haiku about speed.')]);
    print('      User: Write a haiku about speed.');
    print('      Groq: ${response.text}\n');

    // Conversation with context
    print('   Conversation with Context:');
    final conversation = [
      ChatMessage.system(
          'You are a helpful assistant who gives concise, practical answers.'),
      ChatMessage.user('What are the benefits of fast AI responses?'),
    ];

    response = await provider.chat(conversation);
    print('      System: You are a helpful assistant...');
    print('      User: What are the benefits of fast AI responses?');
    print('      Groq: ${response.text}\n');

    // Follow-up question
    conversation.add(ChatMessage.assistant(response.text ?? ''));
    conversation.add(ChatMessage.user('Give me 3 specific examples.'));

    response = await provider.chat(conversation);
    print('      User: Give me 3 specific examples.');
    print('      Groq: ${response.text}');

    print('   ‚úÖ Basic chat demonstration completed\n');
  } catch (e) {
    print('   ‚ùå Basic chat failed: $e\n');
  }
}

/// Demonstrate streaming performance
Future<void> demonstrateStreamingPerformance(String apiKey) async {
  print('üåä Streaming Performance:\n');

  try {
    // Create provider optimized for streaming
    final provider = await ai()
        .groq()
        .apiKey(apiKey)
        .model('llama-3.1-8b-instant')
        .temperature(0.7)
        .maxTokens(400)
        .build();

    final question = 'Write a short story about a robot discovering emotions.';

    print('   Question: $question');
    print('   ü§ñ Groq (streaming): ');

    final stopwatch = Stopwatch()..start();
    var firstChunkTime = 0;
    var chunkCount = 0;
    var totalChars = 0;

    await for (final event
        in provider.chatStream([ChatMessage.user(question)])) {
      switch (event) {
        case TextDeltaEvent(delta: final delta):
          chunkCount++;
          totalChars += delta.length;

          if (firstChunkTime == 0) {
            firstChunkTime = stopwatch.elapsedMilliseconds;
          }

          stdout.write(delta);
          break;

        case CompletionEvent(response: final response):
          stopwatch.stop();
          print('\n');

          if (response.usage != null) {
            print('   üìä Usage: ${response.usage!.totalTokens} tokens');
          }
          break;

        case ErrorEvent(error: final error):
          print('\n   ‚ùå Streaming error: $error');
          return;

        case ThinkingDeltaEvent():
        case ToolCallDeltaEvent():
          break;
      }
    }

    print('\n   ‚ö° Streaming Performance Metrics:');
    print('      ‚Ä¢ Time to first chunk: ${firstChunkTime}ms');
    print('      ‚Ä¢ Total response time: ${stopwatch.elapsedMilliseconds}ms');
    print('      ‚Ä¢ Chunks received: $chunkCount');
    print('      ‚Ä¢ Characters streamed: $totalChars');
    print(
        '      ‚Ä¢ Average chunk size: ${(totalChars / chunkCount).toStringAsFixed(1)} chars');
    print(
        '      ‚Ä¢ Streaming rate: ${(totalChars * 1000 / stopwatch.elapsedMilliseconds).toStringAsFixed(1)} chars/sec');

    print('\n   üöÄ Groq Streaming Benefits:');
    print('      ‚Ä¢ Ultra-fast time to first token');
    print('      ‚Ä¢ Smooth, consistent streaming');
    print('      ‚Ä¢ Excellent for real-time UIs');
    print('      ‚Ä¢ High character throughput');
    print('   ‚úÖ Streaming performance demonstration completed\n');
  } catch (e) {
    print('   ‚ùå Streaming demonstration failed: $e\n');
  }
}

/// Demonstrate best practices
Future<void> demonstrateBestPractices(String apiKey) async {
  print('üèÜ Best Practices:\n');

  // Error handling
  print('   Error Handling:');
  try {
    final provider = await ai()
        .groq()
        .apiKey('invalid-key') // Intentionally invalid
        .model('llama-3.1-8b-instant')
        .build();

    await provider.chat([ChatMessage.user('Test')]);
  } on AuthError catch (e) {
    print('      ‚úÖ Properly caught AuthError: ${e.message}');
  } catch (e) {
    print('      ‚ö†Ô∏è  Unexpected error type: $e');
  }

  // Optimal configuration for speed
  print('\n   Speed-Optimized Configuration:');
  try {
    final speedProvider = await ai()
        .groq()
        .apiKey(apiKey)
        .model('llama-3.1-8b-instant') // Fastest model
        .temperature(0.7) // Balanced creativity
        .maxTokens(300) // Reasonable limit for speed
        .build();

    final response = await speedProvider.chat(
        [ChatMessage.user('Give me 3 quick tips for better productivity.')]);

    print('      ‚úÖ Speed-optimized response: ${response.text}');
  } catch (e) {
    print('      ‚ùå Speed optimization error: $e');
  }

  // Parallel processing for throughput
  print('\n   Parallel Processing:');
  try {
    final provider = await ai()
        .groq()
        .apiKey(apiKey)
        .model('llama-3.1-8b-instant')
        .temperature(0.7)
        .maxTokens(100)
        .build();

    final questions = [
      'What is AI?',
      'What is ML?',
      'What is NLP?',
    ];

    final stopwatch = Stopwatch()..start();

    final futures =
        questions.map((q) => provider.chat([ChatMessage.user(q)])).toList();

    await Future.wait(futures);
    stopwatch.stop();

    print(
        '      ‚úÖ Processed ${questions.length} questions in ${stopwatch.elapsedMilliseconds}ms');
    print(
        '      Average: ${(stopwatch.elapsedMilliseconds / questions.length).toStringAsFixed(1)}ms per question');
  } catch (e) {
    print('      ‚ùå Parallel processing error: $e');
  }

  print('\n   üí° Best Practices Summary:');
  print('      ‚Ä¢ Use llama-3.1-8b-instant for maximum speed');
  print('      ‚Ä¢ Implement streaming for better user experience');
  print('      ‚Ä¢ Keep token limits reasonable for faster responses');
  print('      ‚Ä¢ Use parallel processing for batch operations');
  print('      ‚Ä¢ Handle errors gracefully with proper types');
  print('      ‚Ä¢ Monitor response times and optimize accordingly');
  print('   ‚úÖ Best practices demonstration completed\n');
}

/// üéØ Key Groq Concepts Summary:
///
/// Model Selection:
/// - llama-3.1-8b-instant: Fastest, good for real-time apps
/// - llama-3.1-70b-versatile: Higher quality, still very fast
/// - mixtral-8x7b-32768: Large context, multilingual support
/// - gemma-7b-it: Good instruction following
///
/// Speed Advantages:
/// - Ultra-fast inference (50-200ms typical)
/// - Consistent low latency
/// - High throughput capabilities
/// - Excellent streaming performance
///
/// Best Use Cases:
/// - Real-time chat applications
/// - Interactive assistants
/// - Gaming and entertainment
/// - Live content generation
/// - Voice applications
///
/// Configuration Tips:
/// - Use fastest models for speed-critical apps
/// - Keep token limits reasonable
/// - Implement streaming for better UX
/// - Use parallel processing for throughput
///
/// Next Steps:
/// - speed_optimization.dart: Advanced performance techniques
/// - ../../02_core_features/streaming_chat.dart: Streaming best practices
/// - ../../03_advanced_features/performance_optimization.dart: General optimization
