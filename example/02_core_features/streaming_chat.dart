// ignore_for_file: avoid_print
import 'dart:io';

import 'package:llm_dart_ai/llm_dart_ai.dart';
import 'package:llm_dart_anthropic/llm_dart_anthropic.dart';
import 'package:llm_dart_builder/llm_dart_builder.dart';
import 'package:llm_dart_core/llm_dart_core.dart';
import 'package:llm_dart_groq/llm_dart_groq.dart';
import 'package:llm_dart_openai/llm_dart_openai.dart';

/// üåä Streaming Chat - Real-time Response Streaming
///
/// This example demonstrates how to handle real-time streaming responses:
/// - Processing stream parts as they arrive (recommended: `llm_dart_ai`)
/// - Building responsive user interfaces
/// - Handling different part types
/// - Error recovery and stream management
///
/// Before running, set your API key:
/// export OPENAI_API_KEY="your-key"
/// export GROQ_API_KEY="your-key"
void main() async {
  print('üåä Streaming Chat - Real-time Response Streaming\n');

  registerGroq();
  registerAnthropic();
  registerOpenAI();

  // Get API key
  final apiKey = Platform.environment['GROQ_API_KEY'];
  if (apiKey == null || apiKey.isEmpty) {
    print('‚ùå Please set GROQ_API_KEY environment variable');
    return;
  }

  // Create AI provider (Groq is great for streaming due to speed)
  final provider = await LLMBuilder()
      .provider(groqProviderId)
      .apiKey(apiKey)
      .model('llama-3.1-8b-instant')
      .temperature(0.7)
      .maxTokens(500)
      .build();

  // Demonstrate different streaming scenarios
  await demonstrateBasicStreaming(provider);
  await demonstrateStreamEventTypes(provider);
  await demonstrateStreamingWithThinking(provider);
  await demonstrateStreamErrorHandling(provider);
  await demonstrateStreamPerformance(provider);

  print('\n‚úÖ Streaming chat completed!');
}

/// Demonstrate basic streaming functionality
Future<void> demonstrateBasicStreaming(ChatCapability provider) async {
  print('‚ö° Basic Streaming:\n');

  try {
    final prompt = Prompt(
      messages: [
        PromptMessage.user(
          'Count from 1 to 10 and explain each number briefly.',
        ),
      ],
    );

    print('   User: Count from 1 to 10 and explain each number briefly.');
    print('   AI: ');

    // Stream the response (recommended: llm_dart_ai task API)
    await for (final part in streamText(
      model: provider,
      promptIr: prompt,
    )) {
      switch (part) {
        case TextDeltaPart(delta: final delta):
          stdout.write(delta);
          break;
        case FinishPart():
          print('\n   ‚úÖ Basic streaming successful\n');
          break;
        case ErrorPart(error: final error):
          print('\n   ‚ùå Stream error: $error\n');
          break;
        case ThinkingDeltaPart():
        case ToolCallDeltaPart():
          // Ignore for basic demo.
          break;
      }
    }
  } catch (e) {
    print('   ‚ùå Basic streaming failed: $e\n');
  }
}

/// Demonstrate different streaming part types
Future<void> demonstrateStreamEventTypes(ChatCapability provider) async {
  print('üì° Stream Part Types:\n');

  try {
    final prompt = Prompt(
      messages: [
        PromptMessage.user('Write a short poem about programming.'),
      ],
    );

    print('   User: Write a short poem about programming.');
    print('   Processing parts:\n');

    var textChunks = 0;
    var totalText = '';

    await for (final part in streamChatParts(
      model: provider,
      promptIr: prompt,
    )) {
      switch (part) {
        case LLMTextDeltaPart(:final delta):
          textChunks++;
          totalText += delta;
          print('   üìù Text chunk $textChunks: "$delta"');
          break;

        case LLMReasoningDeltaPart(:final delta):
          print('   üß† Thinking: $delta');
          break;

        case LLMToolCallStartPart(:final toolCall):
          print('   üîß Tool call started: ${toolCall.function.name}');
          break;

        case LLMToolCallDeltaPart(:final toolCall):
          print(
            '   üîß Tool call delta: ${toolCall.function.name} '
            '(args+=${toolCall.function.arguments.length} chars)',
          );
          break;

        case LLMProviderMetadataPart(:final providerMetadata):
          print(
              '   üßæ Provider metadata keys: ${providerMetadata.keys.toList()}');
          break;

        case LLMFinishPart(:final response):
          print('\n   üèÅ Finish part received');
          if (response.usage != null) {
            print('   üìä Usage: ${response.usage!.totalTokens} tokens');
          }
          break;

        case LLMErrorPart(:final error):
          print('   ‚ùå Error part: $error');
          break;

        case LLMTextStartPart():
        case LLMTextEndPart():
        case LLMReasoningStartPart():
        case LLMReasoningEndPart():
        case LLMToolCallEndPart():
        case LLMToolResultPart():
          // Ignore for this demo.
          break;
      }
    }

    print('\n   üìà Stream Statistics:');
    print('      ‚Ä¢ Total text chunks: $textChunks');
    print('      ‚Ä¢ Final text length: ${totalText.length} characters');
    print('   ‚úÖ Part types demonstration successful\n');
  } catch (e) {
    print('   ‚ùå Part types demonstration failed: $e\n');
  }
}

/// Demonstrate streaming with thinking process (if supported)
Future<void> demonstrateStreamingWithThinking(ChatCapability provider) async {
  print('üß† Streaming with Thinking Process:\n');

  try {
    // Try with a provider that supports thinking (switch to Anthropic if available)
    final anthropicKey = Platform.environment['ANTHROPIC_API_KEY'];
    ChatCapability thinkingProvider = provider;

    if (anthropicKey != null && anthropicKey.isNotEmpty) {
      thinkingProvider = await LLMBuilder()
          .provider(anthropicProviderId)
          .apiKey(anthropicKey)
          .model('claude-3-5-haiku-20241022')
          .temperature(0.7)
          .build();
    }

    final prompt = Prompt(
      messages: [
        PromptMessage.user('Solve this step by step: What is 15% of 240?'),
      ],
    );

    print('   User: Solve this step by step: What is 15% of 240?');
    print('   Processing with thinking:\n');

    var hasThinking = false;

    await for (final part in streamText(
      model: thinkingProvider,
      promptIr: prompt,
    )) {
      switch (part) {
        case ThinkingDeltaPart(delta: final delta):
          hasThinking = true;
          print('   üß† Thinking: $delta');
          break;

        case TextDeltaPart(delta: final delta):
          stdout.write(delta);
          break;

        case FinishPart():
          print('\n');
          break;

        case ErrorPart(error: final error):
          print('   ‚ùå Error: $error');
          break;

        case ToolCallDeltaPart():
          // Ignore for this demo.
          break;
      }
    }

    if (hasThinking) {
      print('   ‚úÖ Thinking process captured during streaming');
    } else {
      print(
          '   ‚ÑπÔ∏è  No thinking process available (provider may not support it)');
    }

    print('   ‚úÖ Thinking demonstration completed\n');
  } catch (e) {
    print('   ‚ùå Thinking demonstration failed: $e\n');
  }
}

/// Demonstrate stream error handling
Future<void> demonstrateStreamErrorHandling(ChatCapability provider) async {
  print('üõ°Ô∏è  Stream Error Handling:\n');

  try {
    // Create a provider with invalid settings to trigger errors
    final invalidProvider = await LLMBuilder()
        .provider(openaiProviderId)
        .apiKey('invalid-key') // Invalid API key
        .model('gpt-4o-mini')
        .build();

    final prompt = Prompt(
      messages: [
        PromptMessage.user('This should fail due to invalid API key.'),
      ],
    );

    print('   Testing error handling with invalid API key...');

    await for (final part in streamText(
      model: invalidProvider,
      promptIr: prompt,
    )) {
      switch (part) {
        case TextDeltaPart(delta: final delta):
          print('   üìù Unexpected text: $delta');
          break;

        case ErrorPart(error: final error):
          print('   ‚úÖ Caught error in stream: ${error.runtimeType}');
          print('   üìù Error message: $error');
          break;

        case FinishPart():
          print('   ‚ùå Unexpected completion');
          break;

        case ThinkingDeltaPart():
        case ToolCallDeltaPart():
          // Ignore for this demo.
          break;
      }
    }
  } catch (e) {
    print('   ‚úÖ Caught exception: ${e.runtimeType}');
    print('   üìù Exception message: $e');
  }

  print('\n   üí° Error Handling Best Practices:');
  print('      ‚Ä¢ Always wrap stream processing in try-catch');
  print('      ‚Ä¢ Handle ErrorPart within the stream');
  print('      ‚Ä¢ Implement retry logic for transient errors');
  print('      ‚Ä¢ Provide user feedback for stream interruptions');
  print('   ‚úÖ Error handling demonstration completed\n');
}

/// Demonstrate stream performance characteristics
Future<void> demonstrateStreamPerformance(ChatCapability provider) async {
  print('üöÄ Stream Performance:\n');

  try {
    final prompt = Prompt(
      messages: [
        PromptMessage.user(
          'Write a detailed explanation of machine learning in 200 words.',
        ),
      ],
    );

    print(
        '   User: Write a detailed explanation of machine learning in 200 words.');
    print('   Measuring performance...\n');

    final stopwatch = Stopwatch()..start();
    var firstChunkTime = 0;
    var chunkCount = 0;
    var totalChars = 0;
    final chunkTimes = <int>[];

    await for (final part in streamText(
      model: provider,
      promptIr: prompt,
    )) {
      switch (part) {
        case TextDeltaPart(delta: final delta):
          chunkCount++;
          totalChars += delta.length;

          if (firstChunkTime == 0) {
            firstChunkTime = stopwatch.elapsedMilliseconds;
            print('   ‚ö° First chunk received: ${firstChunkTime}ms');
          }

          chunkTimes.add(stopwatch.elapsedMilliseconds);
          break;

        case FinishPart():
          stopwatch.stop();
          break;

        case ErrorPart(error: final error):
          print('   ‚ùå Performance test error: $error');
          return;

        case ThinkingDeltaPart():
        case ToolCallDeltaPart():
          // Ignore for this demo.
          break;
      }
    }

    final totalTime = stopwatch.elapsedMilliseconds;
    final avgChunkInterval = chunkTimes.length > 1
        ? (totalTime - firstChunkTime) / (chunkTimes.length - 1)
        : 0;

    print('\n   üìä Performance Metrics:');
    print('      ‚Ä¢ Time to first chunk: ${firstChunkTime}ms');
    print('      ‚Ä¢ Total response time: ${totalTime}ms');
    print('      ‚Ä¢ Total chunks: $chunkCount');
    print('      ‚Ä¢ Total characters: $totalChars');
    print(
        '      ‚Ä¢ Average chunk interval: ${avgChunkInterval.toStringAsFixed(1)}ms');
    print(
        '      ‚Ä¢ Characters per second: ${(totalChars * 1000 / totalTime).toStringAsFixed(1)}');

    print('\n   üí° Performance Benefits:');
    print('      ‚Ä¢ Reduced perceived latency (first chunk arrives quickly)');
    print('      ‚Ä¢ Better user experience (progressive content display)');
    print('      ‚Ä¢ Ability to process content as it arrives');
    print('      ‚Ä¢ Early error detection and handling');

    print('   ‚úÖ Performance demonstration completed\n');
  } catch (e) {
    print('   ‚ùå Performance demonstration failed: $e\n');
  }
}

/// üéØ Key Streaming Concepts Summary:
///
/// Recommended stream surface (Vercel-style):
/// - `streamText`: stable legacy-friendly parts (`TextDeltaPart`, `FinishPart`, ...)
/// - `streamChatParts`: richer parts with block boundaries + provider metadata
///
/// Benefits:
/// - Reduced perceived latency
/// - Real-time user feedback
/// - Progressive content display
/// - Better error handling
///
/// Best Practices:
/// 1. Handle all event types appropriately
/// 2. Implement proper error handling
/// 3. Measure and optimize performance
/// 4. Provide user feedback during streaming
/// 5. Consider buffering for UI updates
///
/// Next Steps:
/// - tool_calling.dart: Function calling and execution
/// - structured_output.dart: JSON schema responses
/// - error_handling.dart: Production-ready error management
